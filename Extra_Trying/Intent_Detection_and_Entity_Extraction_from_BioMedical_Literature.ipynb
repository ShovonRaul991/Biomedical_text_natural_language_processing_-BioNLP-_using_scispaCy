{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intent Detection and Entity Extraction from BioMedical Literature\n",
        "\n",
        "## Overview\n",
        "\n",
        "This Python notebook which implements Naming entity extraction of medical data contains code for performing token classification using the Hugging Face Transformers library. It utilizes JSON data and various models provided by the Transformers library for token classification tasks.\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "- Python >= 3.6\n",
        "- transformers \n",
        "- json\n",
        "\n",
        "## Installation\n",
        "\n",
        "Ensure you have the necessary dependencies installed. You can install them using pip:\n",
        "\n",
        "```bash\n",
        "pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh8hj9FBeTSU"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.12.3)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '\"/home/shovonraul991/Documents/M.Tech Assignments/Natural Language Processing/.venv/bin/python\" -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8NTbeIiDnoq",
        "outputId": "e453bb3c-a154-482e-b541-eba8893d468a"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_vrfpr4FE1IN"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Binder-PubMedBERT Token Classification (NER) Documentation\n",
        "\n",
        "## Overview\n",
        "\n",
        "This section of the Python notebook loads the Binder-PubMedBERT tokenizer and model for Named Entity Recognition (NER) tasks. Binder-PubMedBERT is a pre-trained model fine-tuned on biomedical text from PubMed and the MIMIC-III dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoEhjX-E4usn",
        "outputId": "7765cb68-7cfc-4cf0-8154-2ec9a84621e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the model and tokenizer for BINDER-PubMedBERT (NER)\n",
        "binder_tokenizer = AutoTokenizer.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")\n",
        "binder_model = AutoModelForTokenClassification.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTvB_EXx4v85",
        "outputId": "4aea9061-c732-4995-ab97-f4cfd3b6d04e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the model and tokenizer for RoBERTa (Intent Detection)\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "roberta_model = AutoModelForTokenClassification.from_pretrained(\"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ScbgVk84z-Z",
        "outputId": "77bd7cf6-cd2d-4603-b815-485c085b37a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the model and tokenizer for PubMedBERT (NER)\n",
        "pubmed_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
        "pubmed_model = AutoModelForTokenClassification.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AjAWIYNY5GlZ"
      },
      "outputs": [],
      "source": [
        "# Create pipelines for named entity recognition (NER) and intent detection\n",
        "binder_ner_pipeline = pipeline(\"ner\", model=binder_model, tokenizer=binder_tokenizer)\n",
        "pubmed_ner_pipeline = pipeline(\"ner\", model=pubmed_model, tokenizer=pubmed_tokenizer)\n",
        "roberta_intent_pipeline = pipeline(\"ner\", model=roberta_model, tokenizer=roberta_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "576vp85X5Hpl"
      },
      "outputs": [],
      "source": [
        "# # Sample text\n",
        "# paragraph = \"\"\n",
        "# text = \"However, it's important to consult a healthcare professional if symptoms persist or worsen, as they could indicate a more serious underlying condition\"\n",
        "# text4 = \"On the other hand, individuals might ask about the best practices for maintaining a healthy lifestyle, including diet and exercise routines.\"\n",
        "# text3 = \"Additionally, concerns about mental health issues, such as anxiety or depression, are frequently raised, prompting discussions about therapy options or medication management.\"\n",
        "# text1 = \"Pharmacokinetic properties of abacavir were not altered by the addition of either lamivudine or zidovudine.\"\n",
        "# text2 = \"People may also inquire about preventive measures against contagious diseases, especially during flu season or outbreaks.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jD8lAJPHDFD5"
      },
      "outputs": [],
      "source": [
        "def read_txt_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as file:\n",
        "            text = file.read()\n",
        "            return text\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "# file_path = \"input.txt\"  # Change to the path of your text file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHq27az5DL_s",
        "outputId": "1f722083-0dc7-4a7b-966a-77896181a968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text from file:\n",
            "Recent studies have shown promising results in the treatment of pancreatic cancer using targeted therapies. The combination of gemcitabine and nab-paclitaxel has demonstrated improved survival rates in patients with advanced pancreatic adenocarcinoma. Additionally, immunotherapy has emerged as a potential breakthrough in cancer treatment, with checkpoint inhibitors showing efficacy in various malignancies, including melanoma and non-small cell lung cancer. On the other hand, individuals often seek medical advice for common ailments such as the common cold or seasonal allergies. Symptoms like sneezing, coughing, and congestion are commonly associated with these conditions. Over-the-counter remedies such as antihistamines and decongestants are commonly recommended for symptom relief. I love to go to school.\n"
          ]
        }
      ],
      "source": [
        "file_path = \"medical_data.txt\"\n",
        "\n",
        "paragraph = read_txt_file(file_path)\n",
        "if paragraph:\n",
        "    print(\"Text from file:\")\n",
        "    print(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "60IUVUkY5K9Q"
      },
      "outputs": [],
      "source": [
        "def classify_entities_paragraph_pubmed(paragraph):\n",
        "    # Tokenize the paragraph into sentences\n",
        "    sentences = paragraph.split(\". \")\n",
        "\n",
        "    # Initialize a dictionary to store the entities for each sentence\n",
        "    sentence_entities = {}\n",
        "\n",
        "    # Process each sentence separately\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Perform named entity recognition using PubMedBERT\n",
        "        pubmed_ner_results = pubmed_ner_pipeline(sentence)\n",
        "\n",
        "        # Initialize lists to store medical and generic entities for this sentence\n",
        "        medical_entities = []\n",
        "        generic_entities = []\n",
        "\n",
        "        # Classify entities as medical or generic\n",
        "        for entity in pubmed_ner_results:\n",
        "            if entity['score'] > 0.65:  # Adjust score threshold as needed\n",
        "                medical_entities.append(entity['word'])\n",
        "            else:\n",
        "                generic_entities.append(entity['word'])\n",
        "\n",
        "        # Store the classified entities for this sentence\n",
        "        sentence_entities[f\"Sentence {i+1}\"] = {\"Medical Entities\": medical_entities, \"Generic Entities\": generic_entities}\n",
        "\n",
        "    # Return the dictionary containing entities for each sentence\n",
        "    return sentence_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "G5ZGi7kL5t14",
        "outputId": "3dd8b5af-f5e9-4670-9fbd-56fd6acac957"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1:\n",
            "Medical Entities detected by PubMedBERT: []\n",
            "Generic Entities detected by PubMedBERT: ['recent', 'studies', 'have', 'shown', 'promising', 'results', 'in', 'the', 'treatment', 'of', 'pancreatic', 'cancer', 'using', 'targeted', 'therapies']\n",
            "\n",
            "Sentence 2:\n",
            "Medical Entities detected by PubMedBERT: ['pancreatic']\n",
            "Generic Entities detected by PubMedBERT: ['the', 'combination', 'of', 'gemcitabine', 'and', 'nab', '-', 'paclitaxel', 'has', 'demonstrated', 'improved', 'survival', 'rates', 'in', 'patients', 'with', 'advanced', 'adenocarcinoma']\n",
            "\n",
            "Sentence 3:\n",
            "Medical Entities detected by PubMedBERT: []\n",
            "Generic Entities detected by PubMedBERT: ['additionally', ',', 'immunotherapy', 'has', 'emerged', 'as', 'a', 'potential', 'breakthrough', 'in', 'cancer', 'treatment', ',', 'with', 'checkpoint', 'inhibitors', 'showing', 'efficacy', 'in', 'various', 'malignancies', ',', 'including', 'melanoma', 'and', 'non', '-', 'small', 'cell', 'lung', 'cancer']\n",
            "\n",
            "Sentence 4:\n",
            "Medical Entities detected by PubMedBERT: ['for', 'the']\n",
            "Generic Entities detected by PubMedBERT: ['on', 'the', 'other', 'hand', ',', 'individuals', 'often', 'seek', 'medical', 'advice', 'common', 'ail', '##ments', 'such', 'as', 'common', 'cold', 'or', 'seasonal', 'allergies']\n",
            "\n",
            "Sentence 5:\n",
            "Medical Entities detected by PubMedBERT: []\n",
            "Generic Entities detected by PubMedBERT: ['symptoms', 'like', 'sn', '##ee', '##zing', ',', 'cough', '##ing', ',', 'and', 'congestion', 'are', 'commonly', 'associated', 'with', 'these', 'conditions']\n",
            "\n",
            "Sentence 6:\n",
            "Medical Entities detected by PubMedBERT: ['over']\n",
            "Generic Entities detected by PubMedBERT: ['-', 'the', '-', 'counter', 'remed', '##ies', 'such', 'as', 'antih', '##ista', '##mine', '##s', 'and', 'dec', '##onge', '##st', '##ants', 'are', 'commonly', 'recommended', 'for', 'symptom', 'relief']\n",
            "\n",
            "Sentence 7:\n",
            "Medical Entities detected by PubMedBERT: []\n",
            "Generic Entities detected by PubMedBERT: ['i', 'love', 'to', 'go', 'to', 'school', '.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence_entities = classify_entities_paragraph_pubmed(paragraph)\n",
        "\n",
        "# Print the entities for each sentence\n",
        "for sentence, entities in sentence_entities.items():\n",
        "    print(f\"{sentence}:\")\n",
        "    print(\"Medical Entities detected by PubMedBERT:\", entities[\"Medical Entities\"])\n",
        "    print(\"Generic Entities detected by PubMedBERT:\", entities[\"Generic Entities\"])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f30lq0ZE5Qmv"
      },
      "outputs": [],
      "source": [
        "def classify_entities_binder_paragraph(paragraph):\n",
        "    # Tokenize the paragraph into sentences\n",
        "    sentences = paragraph.split(\". \")\n",
        "\n",
        "    # Initialize a dictionary to store the entities for each sentence\n",
        "    sentence_entities = {}\n",
        "\n",
        "    # Process each sentence separately\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Perform named entity recognition using BINDER-PubMedBERT\n",
        "        binder_ner_results = binder_ner_pipeline(sentence)\n",
        "\n",
        "        # Initialize lists to store medical and generic entities for this sentence\n",
        "        medical_entities = []\n",
        "        generic_entities = []\n",
        "\n",
        "        # Classify entities as medical or generic\n",
        "        for entity in binder_ner_results:\n",
        "            if entity['score'] > 0.65:  # Adjust score threshold as needed\n",
        "                medical_entities.append(entity['word'])\n",
        "            else:\n",
        "                generic_entities.append(entity['word'])\n",
        "\n",
        "        # Store the classified entities for this sentence\n",
        "        sentence_entities[f\"Sentence {i+1}\"] = {\"Medical Entities\": medical_entities, \"Generic Entities\": generic_entities}\n",
        "\n",
        "    # Return the dictionary containing entities for each sentence\n",
        "    return sentence_entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YgWWkost5vPm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1:\n",
            "Entities detected by BINDER-PubMedBERT: ['##ra']\n",
            "Generic Entities detected by BINDER-PubMedBERT: ['recent', 'studies', 'have', 'shown', 'promising', 'results', 'in', 'the', 'treatment', 'of', 'pan', '##cre', '##atic', 'cancer', 'using', 'targeted', 'the', '##pies']\n",
            "\n",
            "Sentence 2:\n",
            "Entities detected by BINDER-PubMedBERT: []\n",
            "Generic Entities detected by BINDER-PubMedBERT: ['the', 'combination', 'of', 'gem', '##cit', '##abi', '##ne', 'and', 'na', '##b', '-', 'pac', '##lita', '##x', '##el', 'has', 'demonstrated', 'improved', 'survival', 'rates', 'in', 'patients', 'with', 'advanced', 'pan', '##cre', '##atic', 'aden', '##oca', '##rc', '##ino', '##ma']\n",
            "\n",
            "Sentence 3:\n",
            "Entities detected by BINDER-PubMedBERT: ['##gnan', '##cies', '##ano', 'cell']\n",
            "Generic Entities detected by BINDER-PubMedBERT: ['additionally', ',', 'im', '##mun', '##oth', '##era', '##py', 'has', 'emerged', 'as', 'a', 'potential', 'breakthrough', 'in', 'cancer', 'treatment', ',', 'with', 'checkpoint', 'inhibitors', 'showing', 'efficacy', 'in', 'various', 'mali', ',', 'including', 'mel', '##ma', 'and', 'non', '-', 'small', 'lung', 'cancer']\n",
            "\n",
            "Sentence 4:\n",
            "Entities detected by BINDER-PubMedBERT: [',', '##ents']\n",
            "Generic Entities detected by BINDER-PubMedBERT: ['on', 'the', 'other', 'hand', 'individuals', 'often', 'seek', 'medical', 'advice', 'for', 'common', 'ai', '##lm', 'such', 'as', 'the', 'common', 'cold', 'or', 'seasonal', 'all', '##er', '##gies']\n",
            "\n",
            "Sentence 5:\n",
            "Entities detected by BINDER-PubMedBERT: ['symptoms', 'like', 'congestion', 'associated', 'these']\n",
            "Generic Entities detected by BINDER-PubMedBERT: ['s', '##nee', '##zing', ',', 'coughing', ',', 'and', 'are', 'commonly', 'with', 'conditions']\n",
            "\n",
            "Sentence 6:\n",
            "Entities detected by BINDER-PubMedBERT: []\n",
            "Generic Entities detected by BINDER-PubMedBERT: ['over', '-', 'the', '-', 'counter', 're', '##med', '##ies', 'such', 'as', 'anti', '##his', '##tam', '##ines', 'and', 'deco', '##nge', '##stan', '##ts', 'are', 'commonly', 'recommended', 'for', 'sy', '##mpt', '##om', 'relief']\n",
            "\n",
            "Sentence 7:\n",
            "Entities detected by BINDER-PubMedBERT: []\n",
            "Generic Entities detected by BINDER-PubMedBERT: ['i', 'love', 'to', 'go', 'to', 'school', '.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence_entities = classify_entities_binder_paragraph(paragraph)\n",
        "\n",
        "# Print the entities for each sentence\n",
        "for sentence, entities in sentence_entities.items():\n",
        "    print(f\"{sentence}:\")\n",
        "    print(\"Entities detected by BINDER-PubMedBERT:\", entities[\"Medical Entities\"])\n",
        "    print(\"Generic Entities detected by BINDER-PubMedBERT:\", entities[\"Generic Entities\"])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wDbEao6S5edU"
      },
      "outputs": [],
      "source": [
        "def classify_intent_paragraph(paragraph, model_pipeline, threshold, count):\n",
        "    # Tokenize the paragraph into sentences\n",
        "    sentences = paragraph.split(\". \")\n",
        "\n",
        "    # Initialize a dictionary to store the intent label for each sentence\n",
        "    sentence_intents = {}\n",
        "\n",
        "    # Process each sentence separately\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Perform intent detection using RoBERTa\n",
        "        intent_results = model_pipeline(sentence)\n",
        "\n",
        "        # Classify intent based on the output of RoBERTa pipeline\n",
        "        # Assuming intent_results is a list of dictionaries\n",
        "        intent_label = \"Medical Inquiry\" if sum(token['entity'] == 'LABEL_1' and token['score'] > threshold for token in intent_results) >= count else \"General Inquiry\"\n",
        "\n",
        "\n",
        "        # Store the classified intent label for this sentence\n",
        "        sentence_intents[f\"Sentence {i+1}\"] = intent_label\n",
        "\n",
        "    # Return the dictionary containing intent labels for each sentence\n",
        "    return sentence_intents\n",
        "\n",
        "# # Example usage:\n",
        "# # paragraph = \"Pharmacokinetic properties of abacavir were not altered by the addition of either lamivudine or zidovudine. Recent studies have shown promising results in the treatment of pancreatic cancer using targeted therapies.\"\n",
        "# sentence_intents = classify_intent_roberta_paragraph(paragraph)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uOM8b53W6B5p"
      },
      "outputs": [],
      "source": [
        "def save_intent_data_to_json(paragraph, model_pipeline, filename):\n",
        "    # Tokenize the paragraph into sentences\n",
        "    sentences = paragraph.split(\". \")\n",
        "\n",
        "    # Initialize a list to store JSON-serializable intent data\n",
        "    json_serializable_data = []\n",
        "\n",
        "    # Process each sentence separately\n",
        "    for sentence in sentences:\n",
        "        # Perform intent detection using RoBERTa\n",
        "        intent_results = model_pipeline(sentence)\n",
        "\n",
        "        # Initialize a list to store JSON-serializable token data for this sentence\n",
        "        sentence_data = []\n",
        "\n",
        "        # Iterate over each token in the intent results and extract JSON-serializable information\n",
        "        for token in intent_results:\n",
        "            token_info = {\n",
        "                \"word\": token.get(\"word\", \"\"),\n",
        "                \"entity\": token.get(\"entity\", \"\"),\n",
        "                \"score\": float(token.get(\"score\", 0.0)),  # Convert to float\n",
        "                \"index\": int(token.get(\"index\", 0)),      # Convert to integer\n",
        "                \"start\": int(token.get(\"start\", 0)),      # Convert to integer\n",
        "                \"end\": int(token.get(\"end\", 0))           # Convert to integer\n",
        "            }\n",
        "            sentence_data.append(token_info)\n",
        "\n",
        "        # Append the JSON-serializable token data for this sentence to the list\n",
        "        json_serializable_data.append({\"sentence\": sentence, \"intent_data\": sentence_data})\n",
        "\n",
        "    # Specify the file path to save the JSON file\n",
        "    file_path = f\"{filename}.json\"\n",
        "\n",
        "    # Write JSON data to file\n",
        "    with open(file_path, \"w\") as json_file:\n",
        "        json.dump(json_serializable_data, json_file, indent=4)\n",
        "\n",
        "    print(f\"JSON data has been saved to {file_path}.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON data has been saved to robert_dataset_output.json.\n",
            "roberta_intent_pipeline output: \n",
            "Sentence 1: General Inquiry\n",
            "Sentence 2: General Inquiry\n",
            "Sentence 3: Medical Inquiry\n",
            "Sentence 4: General Inquiry\n",
            "Sentence 5: Medical Inquiry\n",
            "Sentence 6: General Inquiry\n",
            "Sentence 7: General Inquiry\n"
          ]
        }
      ],
      "source": [
        "save_intent_data_to_json(paragraph, roberta_intent_pipeline, \"robert_dataset_output\")\n",
        "\n",
        "sentence_intents = classify_intent_paragraph(paragraph, roberta_intent_pipeline, 0.60, 5)\n",
        "print(\"roberta_intent_pipeline output: \")\n",
        "# Print the intent label for each sentence\n",
        "for sentence, intent_label in sentence_intents.items():\n",
        "    print(f\"{sentence}: {intent_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zkiAflagCX_Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON data has been saved to binder_dataset_output.json.\n",
            "roberta_intent_pipeline output: \n",
            "Sentence 1: General Inquiry\n",
            "Sentence 2: General Inquiry\n",
            "Sentence 3: Medical Inquiry\n",
            "Sentence 4: General Inquiry\n",
            "Sentence 5: General Inquiry\n",
            "Sentence 6: General Inquiry\n",
            "Sentence 7: General Inquiry\n"
          ]
        }
      ],
      "source": [
        "save_intent_data_to_json(paragraph, binder_ner_pipeline, \"binder_dataset_output\")\n",
        "\n",
        "sentence_intents = classify_intent_paragraph(paragraph, binder_ner_pipeline, 0.60, 5)\n",
        "print(\"roberta_intent_pipeline output: \")\n",
        "# Print the intent label for each sentence\n",
        "for sentence, intent_label in sentence_intents.items():\n",
        "    print(f\"{sentence}: {intent_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON data has been saved to pubmed_dataset_output.json.\n",
            "roberta_intent_pipeline output: \n",
            "Sentence 1: Medical Inquiry\n",
            "Sentence 2: General Inquiry\n",
            "Sentence 3: Medical Inquiry\n",
            "Sentence 4: General Inquiry\n",
            "Sentence 5: General Inquiry\n",
            "Sentence 6: General Inquiry\n",
            "Sentence 7: General Inquiry\n"
          ]
        }
      ],
      "source": [
        "save_intent_data_to_json(paragraph, pubmed_ner_pipeline, \"pubmed_dataset_output\")\n",
        "\n",
        "sentence_intents = classify_intent_paragraph(paragraph, pubmed_ner_pipeline, 0.5, 1)\n",
        "print(\"roberta_intent_pipeline output: \")\n",
        "# Print the intent label for each sentence\n",
        "for sentence, intent_label in sentence_intents.items():\n",
        "    print(f\"{sentence}: {intent_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
